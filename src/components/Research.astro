---
import PublicationCard from './PublicationCard.astro';

interface Publication {
  title: string;
  venue: string;
  year: string;
  summary: string;
  authors?: string;
  tags?: string[];
  pdfUrl?: string;
  githubUrl?: string;
  huggingfaceUrl?: string;
  id?: string;
}

interface Props {
  publications?: Publication[];
}

const { publications = [
  {
    title: 'Context is Key: Aligning Large Language Models with Human Moral Judgments through Retrieval-Augmented Generation',
    venue: 'FLAIRS (Florida Artificial Intelligence Research Society)',
    year: '2025',
    summary: 'This research investigates whether pre-trained large language models (LLMs) can align with human moral judgments on a dataset of approximately fifty thousand interpersonal conflicts from the AITA (Am I the A******) subreddit. We introduce a retrieval-augmented generation (RAG) approach that uses pre-trained LLMs as core components. Using OpenAI\'s GPT-4o, our agent outperforms directly prompting the LLM while achieving 83% accuracy and a Matthews correlation coefficient of 0.469 while also reducing the rate of toxic responses from 22.53% to virtually zero.',
    authors: 'Boraske, M.',
    tags: ['LLMs', 'RAG', 'NLP', 'Moral Judgments', 'Conflict Resolution'],
    pdfUrl: 'https://journals.flvc.org/FLAIRS/article/download/138947/144114',
    githubUrl: 'https://github.com/MattBoraske/AITA-RAG-Agent',
    huggingfaceUrl: 'https://huggingface.co/collections/MattBoraske/reddit-aita-finetuning-v2',
  },
  {
    title: 'The Efficacy of Finetuning Large Language Models for Interpersonal Conflict Resolution',
    venue: 'West Chester University of Pennsylvania - M.S. Thesis',
    year: '2024',
    summary: 'This study addresses the gap in evaluating LLMs on ambiguous tasks such as interpersonal conflict resolution. We evaluate LLMs on four new datasets derived from the "Am I the A**hole" (AITA) subreddit, featuring discussions of interpersonal conflicts. Using Google\'s Flan-T5 and Meta\'s Llama-2-Chat, these models were evaluated on their ability to classify and justify conflicts and their tendency to generate toxic language. Findings suggest that the most effective strategy involves finetuning an encoder-decoder LLM on a dataset cleaned of toxicity, followed by iterative refinement using RLHF to align with ethical standards.',
    authors: 'Boraske, M. (Advised by Dr. Richard Burns)',
    tags: ['LLMs', 'Fine-tuning', 'NLP', 'Conflict Resolution', 'RLHF'],
    pdfUrl: 'https://digitalcommons.wcupa.edu/cgi/viewcontent.cgi?article=1461&context=all_theses',
    githubUrl: 'https://github.com/MattBoraske/Reddit-AITA-Conflict-Resolution-LLM-FT-Efficacy',
    huggingfaceUrl: 'https://huggingface.co/collections/MattBoraske/reddit-aita-finetuning-v1',
    id: 'thesis',
  },
]} = Astro.props;
---

<section id="research" class="py-24 px-6">
  <div class="max-w-content mx-auto">
    <div class="section-header animate-on-scroll">
      <span class="header-bracket">[</span>
      <h2 class="text-3xl md:text-4xl font-bold text-text">Research</h2>
      <span class="header-bracket">]</span>
    </div>

    <div class="grid grid-cols-1 md:grid-cols-2 gap-6">
      {publications.map((pub) => (
        <PublicationCard {...pub} />
      ))}
    </div>
  </div>
</section>

<style>
  .section-header {
    display: flex;
    align-items: center;
    justify-content: center;
    gap: 1rem;
    margin-bottom: 3rem;
  }

  .header-bracket {
    font-family: var(--font-display);
    font-size: 3rem;
    color: var(--color-accent);
    font-weight: 900;
  }
</style>
